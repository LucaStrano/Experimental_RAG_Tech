{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07dd9068",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LucaStrano/Experimental_RAG_Tech/blob/main/experimental_tech/2_compress_and_rerank.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5300c",
   "metadata": {},
   "source": [
    "## Single Pass Rerank and Contextual Compression using Recursive Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ff636",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook demonstrates how to use a **Reranker Model** to perform both _Reranking_ and _Contextual Compression_ in a single pass. We will go over the **Intuition** behind the technique, the **Implementation** details and, finally, a short **Conclusion**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a734b8",
   "metadata": {},
   "source": [
    "### 1. Intuition\n",
    "\n",
    "The use of a Reranker Model has become essential in most modern RAG pipelines, especially when dealing with large or complex datasets. It significantly improves the **Precision** of retrieved results by re-evaluating and reordering initial retrieved documents based on deeper semantic understanding. A Reranker Model is tipically involved in the following steps:\n",
    "\n",
    "1. Use a fast **Embedding Model** to retrieve a set of $\\text{top}_K$ candidate documents of based on query similarity. Usually, $\\text{top}_K$ is set to a relatively high number to ensure high **Recall**;\n",
    "\n",
    "2. Use a **Reranker Model** to re-evaluate the $\\text{top}_K$ documents and select the $top_N$ most relevant ones. $\\text{top}_N$ is usually set to a much lower number to ensure high **Precision**.\n",
    "\n",
    "You might wonder why a reranker model is necessary at all: after all, the initial retrieval step already returns a set of seemingly relevant documents. This is because embedding models, while effective for initial retrieval, rely on the **Encoder** Architecture which compresses the semantic meaning of the documents into fixed-size vectors. Relevance is then estimated using a _similarity function_ (such as cosine similarity) over the calculated vectors. While this approach is efficient, it can miss subtle semantic nuances and contextual cues of the original texts.\n",
    "Reranker Models, on the other hand, use a **Cross-Encoder** architecture, which jointly processes the query and candidate documents at the _token level_, allowing for a more fine-grained understanding of their relationship. This process, while more computationally expensive, ensures a higher quality of the final results.\n",
    "\n",
    "To further enhance the quality of the results, we can also apply a **Contextual Compression** step. This step involves breaking down the retrieved documents into smaller, more manageable chunks. This allows us to not only select the most relevant documents but also to extract only the most relevant pieces from them, effectively compressing the context while retaining essential information.\n",
    "\n",
    "The problem with this pipeline is that it now requires three separate steps: An initial retrieval step, a reranking step, and a compression step. Using traditional methods, this can be inefficient and highly time-consuming. What if we could combine both Reranking and Compression into a single step? This is where the **Recursive Reranking** technique comes into play, which functions as follows:\n",
    "\n",
    "1. Use a fast Embedding Model to retrieve a set of $top_K$ candidate documents;\n",
    "\n",
    "2. Using a Reranker Model, calculate a relevance score for each sub-section of each document;\n",
    "\n",
    "3. Use calculated sub-section scores to both rerank documents and select only the most relevant sub-sections of each reranked document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89054d33",
   "metadata": {},
   "source": [
    "### 2. Recursive Reranking\n",
    "\n",
    "This section focuses on the **Preliminaries** and the **Implementation** of the Recursive Reranking Technique.\n",
    "\n",
    "### 2.1 Preliminaries to Recursive Reranking\n",
    "\n",
    "Let's start by installing the necessary dependencies. We will use the `chromadb` library to handle our vector database, and the `sentence-transformers` library to use our Reranker Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db373da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge sentence-transformers hf-xet chromadb\n",
    "# %pip install -U sentence-transformers hf-xet chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d71d2",
   "metadata": {},
   "source": [
    "Let's first define our example documents that we will use throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e0e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"\"\"\n",
    "Italy, officially the Italian Republic, is a country in Southern and Western Europe.\n",
    "It consists of a peninsula that extends into the Mediterranean Sea.\n",
    "The Alps mountain range forms its northern boundary, while the Apennine Mountains run down the length of the peninsula.\n",
    "The territory also includes well as nearly 800 islands, notably Sicily and Sardinia.\n",
    "It is a country in Southern Europe with a population of approximately 60 million people.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "The capital of Italy is Rome, which is also the largest city in the country.\n",
    "Rome is known for its nearly 3,000 years of globally influential art, architecture, and culture.\n",
    "The city is often referred to as the \"Eternal City\" and is famous for its ancient history, including landmarks such as the Colosseum and the Vatican.\n",
    "It is the capital city of Italy and has a population of almost 3 million people.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "Italy's history goes back to numerous Italic peoplesâ€”notably including the ancient Romans, \n",
    "who conquered the Mediterranean world during the Roman Republic and ruled it for centuries during the Roman Empire.\n",
    "The Roman Empire was among the largest in history, wielding great economical, cultural, political, and military power.\n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "France is a country in Western Europe, known for its rich history, culture, and influence.\n",
    "The food in France is renowned worldwide, with dishes like coq au vin and ratatouille.\n",
    "France has a world class cuisine and is famous for its wine, cheese, and pastries.\n",
    "Regions like Bordeaux and Champagne are particularly well-known in the culinary world.\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36c0b3",
   "metadata": {},
   "source": [
    "We will use Chroma's in-memory vector database to simulate the initial retrieval step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from uuid import uuid4\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"italy\")\n",
    "collection.add(\n",
    "    ids = [str(uuid4()) for _ in range(len(docs))],\n",
    "    documents = docs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7ec4",
   "metadata": {},
   "source": [
    "Chroma automatically handles the creation of embeddings for the documents we add to the collection. By default, it uses the [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model, which is lightweight and efficient. Let's try querying our collection to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87caa818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: \n",
      "The capital of Italy is Rome, which is also the largest city in the country.\n",
      "Rome is known for its nearly 3,000 years of globally influential art, architecture, and culture.\n",
      "The city is often referred to as the \"Eternal City\" and is famous for its ancient history, including landmarks such as the Colosseum and the Vatican.\n",
      "It is the capital city of Italy and has a population of almost 3 million people.\n",
      "\n",
      "ID: 9b23cca7-2749-4ed1-9404-4edd035a1b8a\n",
      "Distance: 0.6291064023971558\n",
      "--------------------------------------------------\n",
      "Document 2: \n",
      "Italy, officially the Italian Republic, is a country in Southern and Western Europe.\n",
      "It consists of a peninsula that extends into the Mediterranean Sea.\n",
      "The Alps mountain range forms its northern boundary, while the Apennine Mountains run down the length of the peninsula.\n",
      "The territory also includes well as nearly 800 islands, notably Sicily and Sardinia.\n",
      "It is a country in Southern Europe with a population of approximately 60 million people.\n",
      "\n",
      "ID: 05b94dfd-010c-426d-8add-bd427066cfff\n",
      "Distance: 0.7050184011459351\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"How many people live in Italy and what is the capital?\"\n",
    "results = collection.query(query_texts=[query], n_results=2)\n",
    "\n",
    "for i, id in enumerate(results['ids'][0]):\n",
    "    print(f\"Document {i+1}: {results['documents'][0][i]}\")\n",
    "    print(f\"ID: {id}\\nDistance: {results['distances'][0][i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38fa59",
   "metadata": {},
   "source": [
    "We get great results. Let's now introduce our Reranker Model. We will use the [`cross-encoder/ms-marco-MiniLM-L-6-v2`](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2) model, which is a lightweight Cross-Encoder model designed for reranking tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461388ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "âœ… Reranker model loaded.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import torch # comes with sentence-transformers\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() \\\n",
    "         else 'mps' if torch.backends.mps.is_available() \\\n",
    "         else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "rerank = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2', device=DEVICE)\n",
    "print(\"âœ… Reranker model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a78eb8",
   "metadata": {},
   "source": [
    "Let's do a quick check and see how the model ranks our documents with an example query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e18fd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5755582,  9.614823 , -4.1138754, -8.50889  ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the capital of Italy?\"\n",
    "rerank_results = rerank.predict([[query, doc] for doc in docs])\n",
    "rerank_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06327b",
   "metadata": {},
   "source": [
    "Unsurprisingly, the highest ranked document is the one that talks about Rome, which is the capital of Italy. The lowest ranked document is the one that talks about France, which is not relevant to the query at all. This is to be expected. \n",
    "Letâ€™s now take the highest-ranked document and predict a relevance score for each of its sentences individually. This allows us to analyze the alignment between the query and different parts of the document in a more fine-graned way, rather than treating the whole document as a single block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b804149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: The capital of Italy is Rome, which is also the largest city in the country\n",
      "Score: 9.2809\n",
      "--------------------------------------------------\n",
      "Sentence 2: Rome is known for its nearly 3,000 years of globally influential art, architecture, and culture\n",
      "Score: -1.6989\n",
      "--------------------------------------------------\n",
      "Sentence 3: The city is often referred to as the \"Eternal City\" and is famous for its ancient history, including landmarks such as the Colosseum and the Vatican\n",
      "Score: -7.3384\n",
      "--------------------------------------------------\n",
      "Sentence 4: It is the capital city of Italy and has a population of almost 3 million people\n",
      "Score: 6.3446\n",
      "--------------------------------------------------\n",
      "Mean score of sentences: 1.6470724\n",
      "Standard deviation of scores: 6.5626807\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of Italy?\"\n",
    "sents = [sent.strip() for sent in docs[1].split('.') if sent.strip()]\n",
    "sent_scores = rerank.predict([[query, sent] for sent in sents])\n",
    "\n",
    "for i, (sent, score) in enumerate(zip(sents, sent_scores)):\n",
    "    print(f\"Sentence {i+1}: {sent}\")\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Mean score of sentences:\", sent_scores.mean())\n",
    "print(\"Standard deviation of scores:\", sent_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783bb5c",
   "metadata": {},
   "source": [
    "We can see that the model assigns the highest score to the sentence that is more relevant to the query. The mean score of the whole document is quite low, given that the document contains multiple sentences that are not strictly relevant to the query. This is a common issue with long documents, where an high portion of sentences may not be relevant at all. The standard deviation of the scores is also quite high. We should control for these statistical measures when performing the selection of both document and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a133ed",
   "metadata": {},
   "source": [
    "### 2.2 Implementing Recursive Reranking\n",
    "\n",
    "We are all set up! We can now implement the main logic of our Recursive Reranking Technique. The approach consists of the following steps:\n",
    "\n",
    "1. Split each document into separate sentences, then use the Reranker Model to calculate a relevance score for each sentence with respect to the query;\n",
    "\n",
    "2. Each document is then given a score based on the mean of the highest $\\text{score}_N$ scores of its sentences (This is done because we could have long chunks that contain multiple non-relevant sentences, which can drag down the overall score of the document);\n",
    "\n",
    "3. Select the $\\text{top}_N$ documents based on their scores;\n",
    "\n",
    "4. For each selected document, we perform Contextual Compression by selecting only the most relevant sentences using a simple **Static Filter**. Specifically, for document $d$, we select all sentences whose score satisfies:\n",
    "$$\\text{score} \\geq \\mu_d + \\alpha \\cdot \\sigma_d$$\n",
    "where $\\mu_d$ is the mean and $\\sigma_d$ is the standard deviation of sentence scores in document $d$, and $\\alpha$ is a tunable hyperparameter controlling the strictness of the filter. \n",
    "\n",
    "We start by defining a function that will perform the initial retrieval step using the Chroma collection we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcf4a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query : str, k : int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k documents from the Chroma collection based on the query.\n",
    "    \"\"\"\n",
    "    results = collection.query(query_texts=[query], n_results=k)\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc28c20",
   "metadata": {},
   "source": [
    "Let's now define the main hyperparamters and implement the `recursive_rerank` function, which takes a `query` as a string and `docs` as a list of strings, and returns a list of reranked and compressed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SCORE_N = 2     # Number of top sentences to consider for document scoring\n",
    "TOP_N = 2       # Number of top documents to select\n",
    "ALPHA = 0.2     # Strength of Contextual Compression\n",
    "\n",
    "def recursive_rerank(query: str, \n",
    "                     docs: list[str],\n",
    "                     score_n : int = SCORE_N,\n",
    "                     top_n : int = TOP_N,\n",
    "                     alpha : float = ALPHA) -> list[str]:\n",
    "    \"\"\"\n",
    "    Perform recursive reranking and contextual compression of documents in a single pass.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate sentence scores\n",
    "    all_sents = []\n",
    "    sent_scores = []\n",
    "    for doc in docs:\n",
    "        # Split using SpaCy for better sentence segmentation\n",
    "        sents = [sent.strip() for sent in doc.split('.') if sent.strip()]\n",
    "        all_sents.append(sents)\n",
    "        scores = rerank.predict([[query, sent] for sent in sents])\n",
    "        sent_scores.append(scores)\n",
    "\n",
    "    # Step 2: Calculate document scores based on top score_N sentence scores\n",
    "    doc_scores = []\n",
    "    for scores in sent_scores:\n",
    "        indx = min(score_n, len(scores))\n",
    "        sorted_scores = sorted(scores, reverse=True)[:indx]\n",
    "        doc_score = sum(sorted_scores) / indx\n",
    "        doc_scores.append(doc_score)\n",
    "\n",
    "    # Step 3: Select top N documents\n",
    "    # We will use document indices to save space\n",
    "    top_docs_indices = \\\n",
    "        sorted(\n",
    "            range(len(doc_scores)), \n",
    "            key=lambda i: doc_scores[i], \n",
    "            reverse=True\n",
    "        )[:min(top_n, len(doc_scores))]\n",
    "    \n",
    "    # Step 4: rerank and compress documents whose indices are in top_docs_indices\n",
    "    filtered_docs = []\n",
    "    for i in top_docs_indices:\n",
    "        mean = np.mean(sent_scores[i])\n",
    "        std_dev = np.std(sent_scores[i])\n",
    "        filtered_sents = [sent for sent, score in zip(all_sents[i], sent_scores[i]) \n",
    "                          if score >= mean + alpha * std_dev]\n",
    "        if filtered_sents:\n",
    "            filtered_docs.append('.\\n'.join(filtered_sents) + '.')\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac4aea",
   "metadata": {},
   "source": [
    "Let's finally test our implementation with an example query and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ffa9c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Italy is Rome, which is also the largest city in the country.\n",
      "It is the capital city of Italy and has a population of almost 3 million people.\n",
      "--------------------------------------------------\n",
      "Italy, officially the Italian Republic, is a country in Southern and Western Europe.\n",
      "It is a country in Southern Europe with a population of approximately 60 million people.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retrieve_query = \"How many people live in Italy and what is the capital?\"\n",
    "docs = retrieve(retrieve_query, k=4)\n",
    "reranked_docs = recursive_rerank(retrieve_query, docs)\n",
    "for doc in reranked_docs:\n",
    "    print(doc)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32fecb",
   "metadata": {},
   "source": [
    "We get exactly what we want! The two reranked documents returned are the ones discussing Rome and the population of Italy, which are both aligned with the query. We also retained only the most relevant sentences from each document, effectively performing Contextual Compression. Let's test it once again with a different query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e889b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food in France is renowned worldwide, with dishes like coq au vin and ratatouille.\n",
      "France has a world class cuisine and is famous for its wine, cheese, and pastries.\n",
      "--------------------------------------------------\n",
      "Italy, officially the Italian Republic, is a country in Southern and Western Europe.\n",
      "It is a country in Southern Europe with a population of approximately 60 million people.\n",
      "--------------------------------------------------\n",
      "The capital of Italy is Rome, which is also the largest city in the country.\n",
      "It is the capital city of Italy and has a population of almost 3 million people.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retrieve_query = \"Does france have good food?\"\n",
    "docs = retrieve(retrieve_query, k=4)\n",
    "reranked_docs = recursive_rerank(retrieve_query, docs, alpha=0.2, top_n=3)\n",
    "for doc in reranked_docs:\n",
    "    print(doc)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb71a5",
   "metadata": {},
   "source": [
    "This time, the highest ranked document is the one that talks about France and its food. We can also control the strictness of the Contextual Compression step by adjusting the `alpha` parameter. Let's try the same query with an higher `alpha` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25e07369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food in France is renowned worldwide, with dishes like coq au vin and ratatouille.\n",
      "France has a world class cuisine and is famous for its wine, cheese, and pastries.\n",
      "--------------------------------------------------\n",
      "It is a country in Southern Europe with a population of approximately 60 million people.\n",
      "--------------------------------------------------\n",
      "The capital of Italy is Rome, which is also the largest city in the country.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retrieve_query = \"Does france have good food?\"\n",
    "docs = retrieve(retrieve_query, k=4)\n",
    "reranked_docs = recursive_rerank(retrieve_query, docs, alpha=0.9, top_n=3)\n",
    "for doc in reranked_docs:\n",
    "    print(doc)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c7df2",
   "metadata": {},
   "source": [
    "As you can see, The highest ranked document has still retained every sentence, but the other documents have been compressed to only one sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cc9a7",
   "metadata": {},
   "source": [
    "### 3. Conclusion\n",
    "\n",
    "You can find a diagram of the Recursive Reranking Technique [At this link](https://raw.githubusercontent.com/LucaStrano/Experimental_RAG_Tech/refs/heads/main/images/2_compress_and_rerank_diagram.png).\n",
    "\n",
    "The Recursive Reranking Technique offers a powerful way to combine both Reranking and Contextual Compression in a single pass. This technique is particularly useful when dealing with noisy chunks and high retrieval hyperparameters. \n",
    "\n",
    "The main advantages of this approach include:\n",
    "\n",
    "- High efficiency, since it combines both Reranking and Contextual Compression in a single pass;\n",
    "\n",
    "- Lower latency, since it avoids the need of perfoming multiple LLM calls to compress the retrieved documents.\n",
    "\n",
    "This technique works best when paired with other chunking techniques such as **Semantic Chunking** or **Proposition Chunking**. The Recursive Reranking function could also be enhanced by using a better (but more computationally intesive) Reranker Model or by considering windows of sentences instead of single sentences. This would allow for a wider understanding of the context, especially with ambiguous sentences where entities aren't directly mentioned (e.g., _The capital of Italy is rome. It is a city containing..._)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".exp_rag_tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
