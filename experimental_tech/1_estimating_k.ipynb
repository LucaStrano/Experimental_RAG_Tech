{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f93758",
   "metadata": {},
   "source": [
    "## Estimating the K hyperparamter using Query Complexity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2797d",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook provides an explanation on how to dynamically estimate the **K hyperparameter** (number of docs to retrieve) during the retrieval phase using the **Query Complexity Score** (QCS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6ffa8",
   "metadata": {},
   "source": [
    "### 1. Intuition\n",
    "\n",
    "When building a comprehensive RAG system, one of the most important hyperparameters to tune is **K**, which represents the number of documents (before reranking, if any) to retrieve for each query. The choice of this hyperparameter can significantly impact the performances of the system, especially in terms of **precision** and **answer feasibility**:\n",
    " - A **low K** will lead to _higher precision_ (since the number of retrieved documents is low), but may result in missing relevant information, which leads to incomplete answers;\n",
    "  \n",
    " - A **high K** will lead lo _lower precision_, but may provide more relevant information, which leads to more complete answers.\n",
    "\n",
    "Unfortunately, **not all queries are created equal**: Some queries are more complex than others, and therefore may require more documents to be retrieved (higher K) in order to provide a complete answer. Other queries may be simpler and therefore require less documents (lower K) to achieve a satisfactory answer. For example, the query \"What's the capital of Italy?\" is a simple one and can be answered with a single chunk that contains the phrase \"Rome is the capital of Italy\"; on the other hand, the query \"What are the main differences between Italian and French cusine?\" is more complex and may require multiple chunks to be retrieved in order to provide an answer.\n",
    "\n",
    "This leads to the problem of **estimating the K hyperparameter** for each query, which is not a simple task. What if we could associate a score to each query that roughly reflects its complexity and, based on that score, estimate the K value to use for that query so that higher complexity scores lead to higher K values? This is the intution behind the **Query Complexity Score** (QCS), which is discussed in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b746c9",
   "metadata": {},
   "source": [
    "### 2. Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da88cbc",
   "metadata": {},
   "source": [
    "#### 2.1 High Static K\n",
    "\n",
    "The simplest approach to _\"solve\"_ this problem is setting an high, fixed K value. If the varying range of complexity of the queries that our system is going to receive is known a priori, a K value that is high enough can be fixed to provide a satisfactory answer for the most complex queries.\n",
    "As already discussed, this approach has the drawback of leading to lower precision as well as higher costs and latency even for the simplest queries.\n",
    "\n",
    "> Please note that these problems could be somewhat mitigated by using a **reranker** model on the retrieved chunks, but this notebook focuses on another approach entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c7be1",
   "metadata": {},
   "source": [
    "#### 2.2 Training a Model to Estimate K for potential queries\n",
    "\n",
    "Another approach is to train a model to predict the value of K for a given query. This model can be trained on a dataset of queries and their corresponding K values, which can be obtained by generating syntethic queries of varying complexity and manually annotating them with an appropiate value of K. This approach works well if the training dataset is highly representative of the queries that the system is going to receive. However, it has the drawback of requiring a good dataset (which is difficult to obtain, especially with syntethic queries) and a good model that is able to generalize well to unseen queries. However, this approach is costly, time-consuming, and requires effort to maintain the model up-to-date in a system that is constantly evolving.\n",
    "\n",
    "> For a thorough implementation of this approach, you can check out this [Medium Article](https://medium.com/@sauravjoshi23/optimizing-retrieval-augmentation-with-dynamic-top-k-tuning-for-efficient-question-answering-11961503d4ae) by Saurav Joshi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da6c76",
   "metadata": {},
   "source": [
    "#### 2.3 Ask an LLM\n",
    "\n",
    "Of course, the **JALM** (Just Ask a Language Model) approach is always an option and, in most cases, the best one in terms of quality. a smart-enough LLM could be able to estimate the K value for a given query based on its complexity and, optionally, the context of the system.\n",
    "Another approach relies on the use of **Query Composition**: Starting from the original query the LLM is _kindly_ asked to generate a set of small, atomic queries (addionally each with an associated K value) that reflects the decomposition of the original query into smaller, more manageable parts. The retrieval results for each sb-query is then merged using techniques like **Reciprocal Rank Fusion** or, optionally, each sub-query is answered separately and the sub-results are merged. This approach is particularly useful when the original query is too complex to be answered in a single step, but it requires a good LLM and adds a lot of complexity and latency to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846471da",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Query Complexity Score\n",
    "\n",
    "To estimate the **Complexity Score** for a given query, we will use different heurstics:\n",
    "- **Length of the query**: The longer the query (in terms of _tokens_), the more complex it is likely to be;\n",
    "- **Number of different entities in the query**: The more entities are mentioned in the query, the more complex it is likely to be;\n",
    "- **Number of different sentences or conjunctions in the query**: The more sentences or conjunctions are used in the query, the more complex it is likely to be;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8e20a",
   "metadata": {},
   "source": [
    "#### 3.1 Implementing QCS\n",
    "\n",
    "To implement a function that calculates the QCS for a given query, we will take advantage of the [**SpaCy** library](https://spacy.io/), which provides different powerful NLP pipelines to perform the needed operations. In particular, we will use the **en_core_web_sm** pipeline, which is blazingly fast and amounts to ~12 MB of disk space.\n",
    "\n",
    "Here is what we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d37fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install --yes spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31700ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "except IOError:\n",
    "\tfrom spacy.cli import download\n",
    "\tdownload(\"en_core_web_sm\")\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e5a0d",
   "metadata": {},
   "source": [
    "Now that we've loaded the SpaCy models, we can try them out on a test query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Rome is the capital of Italy. Italy is known for its rich history, art (e.g. David by Michelangelo), and culture.\n",
      "********************\n",
      "Number of tokens: 26\n",
      "--------------------\n",
      "Number of sentences: 2\n",
      " Sentence 2 contains CONJUCTION ('and')\n",
      "--------------------\n",
      "Number of entities: 5\n",
      " 1. Entity: Rome, Label: GPE\n",
      " 2. Entity: Italy, Label: GPE\n",
      " 3. Entity: Italy, Label: GPE\n",
      " 4. Entity: David, Label: PERSON\n",
      " 5. Entity: Michelangelo, Label: PERSON\n"
     ]
    }
   ],
   "source": [
    "test = \"Rome is the capital of Italy. Italy is known for its rich history, art (e.g. David by Michelangelo), and culture.\"\n",
    "doc = nlp(test)\n",
    "\n",
    "print(f\"Query: {test}\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Number of tokens: {len(doc)}\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Number of sentences: {len(list(doc.sents))}\")\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    if \"and\" in sent.text:\n",
    "        print(f\" Sentence {i+1} contains CONJUCTION ('and')\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Number of entities: {len(doc.ents)}\")\n",
    "for i, ent in enumerate(doc.ents):\n",
    "    print(f\" {i+1}. Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe4df8",
   "metadata": {},
   "source": [
    "We get exactly what we need. Before we proceed, let's go over some observations:\n",
    "- The **query length** is a good-ish indicator of query complexity, But it is not enough on its own: for example, the query \"_What's the capital of the country next to France_?\" isn't long, but its ambiguous and it requires some reasoning to provide an answer. Another point to consider is that we are using _number of tokens_ as a measure of length instead of _number of characters_. This is because the QCS is going to be a _weighted average_ of the different components, and considering number of tokens as a length measure allows us to have a slightly more consistent measure across queries;\n",
    "  \n",
    "- We are using the number of **Distinct** entities in the query, not the total number of entities. This is because we could have queries such as \"Where's Italy and what's the capital of Italy?\" that have the same entity mentioned multiple times, but since we are also considering the number of distinct sentences, this repetition shouldn't be considered while calculating the QCS;\n",
    "  \n",
    "- **Sentence Segmentation** is done smartly: In the example query, we have the \"_e.g. David by Michelangelo_\" that, while containing dots, isn't considered as separate sentences;\n",
    "  \n",
    "- Estimating the number of **Conjunctions** in a query is the trickiest part. We can't just count how many times the token \"_and_\" appears in the query. For example, the query \"What are the Q3 earnings of Johnson and Johnson?\" contains a conjunction, but it's part of a company name, so it shouldn't add complexity. What we ultimately want to count is the number of **Coordinating Conjunctions** that connect main clauses in the query. We can use **Dependency Parsing** to achieve this.\n",
    "\n",
    "> Please note that to calculate the QCS we are only considering the \"and\" token, but the set of coordinating conjunctions consists of the _FANBOYS_ conjunctions: **For**, **And**, **Nor**, **But**, **Or**, **Yet**, **So**. This is a matter of preference that can be adjusted based on the specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020e9c8",
   "metadata": {},
   "source": [
    "Now that we finally have the full intuition behind the QCS, we can start implementing it. The `calculate_qcs` function takes a `query`string as input and returns the QCS as a `float` between 0 and 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".exp_rag_tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
