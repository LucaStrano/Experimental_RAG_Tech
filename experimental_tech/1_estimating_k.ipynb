{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f93758",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LucaStrano/Experimental_RAG_Tech/blob/main/experimental_tech/1_estimating_k.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20250bd1",
   "metadata": {},
   "source": [
    "## Dynamically Estimating the K (Hyper)parameter using Query Complexity Score (QCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2797d",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook provides a thorough explanation on how to dynamically estimate the **K parameter** (number of docs to retrieve) during the retrieval phase using the **Query Complexity Score** (QCS). We will go over The **Intuition** behind the Query Complexity Score, **Alternative Solutions** to the problem, the **Implementation** of the QCS function and finally a short **Conclusion**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6ffa8",
   "metadata": {},
   "source": [
    "### 1. Intuition\n",
    "\n",
    "When building a comprehensive RAG system, one of the most important hyperparameters to tune is **K**, which represents the number of documents (before reranking, if any) to retrieve for each query. The choice of this hyperparameter can significantly impact the performances of the system, especially in terms of **precision** and **answer feasibility**:\n",
    "\n",
    " - A **low K** will lead to _higher precision_ (since the number of retrieved documents is low), but may result in missing relevant information, which leads to incomplete answers;\n",
    "  \n",
    " - A **high K** will lead lo _lower precision_, but may provide more relevant information, which leads to more complete answers.\n",
    "\n",
    "Unfortunately, **not all queries are created equal**: Some queries are more complex than others, and therefore may require more documents to be retrieved (higher K) in order to provide a complete answer. Other queries may be simpler and therefore require less documents (lower K) to achieve a satisfactory answer. For example, the query \"_What's the capital of Italy?_\" is trivial and can be answered by retrieving a single chunk; on the other hand, the query \"_What is the capital of Italy and what can i visit There? What are the main attractions?_\" is more complex and may require to fetch multiple chunks in order to provide an answer.\n",
    "\n",
    "Given the wide variety of queries a RAG system can receive, it’s intuitive to prefer a **dynamic K**, making it no longer an _hyperparameter_ of the system. What if we could assign each query a score that (roughly) reflects its complexity, and use this score to estimate how many documents to retrieve? Ideally, complex queries should lead higher scores, which in turn lead to higher K values, allowing the system to retrieve more documents from the Knowledge Base. This is the intution behind the **Query Complexity Score** (QCS), which we will explore in detail in the following chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b746c9",
   "metadata": {},
   "source": [
    "### 2. Alternative Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da88cbc",
   "metadata": {},
   "source": [
    "#### 2.1 High Static K\n",
    "\n",
    "The simplest approach to _\"solve\"_ this problem is setting an high, fixed K value. If we already know the expected range of query complexity, We could fix a K value that is high enough to provide a satisfactory answer for even the most complex queries.\n",
    "As already discussed, this approach has the drawback of leading to lower precision as well as higher costs and latency even for the simplest queries.\n",
    "\n",
    "> Please note that these problems could be somewhat mitigated by using a **reranker** model on the retrieved chunks, but this notebook focuses on another approach entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c7be1",
   "metadata": {},
   "source": [
    "#### 2.2 Training a Model to Estimate K for potential queries\n",
    "\n",
    "Another plausible approach is to train a model to predict the value of K for a given query. This model can be trained on a dataset of queries and their corresponding K values, which can be obtained by generating syntethic queries of varying complexity and manually annotating them with an appropiate value of K. This approach works well if the training dataset is highly representative of the queries that the system is going to receive. However, it has the drawback of requiring a good dataset (which is difficult to obtain, especially with syntethic queries) and a good model that is able to generalize well to unseen queries. This approach is also costly, time-consuming, and requires effort to maintain the model up-to-date in a system that is constantly evolving.\n",
    "\n",
    "> For a thorough implementation of this approach, you can read this [Medium Article](https://medium.com/@sauravjoshi23/optimizing-retrieval-augmentation-with-dynamic-top-k-tuning-for-efficient-question-answering-11961503d4ae) by Saurav Joshi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da6c76",
   "metadata": {},
   "source": [
    "#### 2.3 Ask an LLM\n",
    "\n",
    "Of course, the **JALM** (Just Ask a Language Model) approach is always an option and, in most cases, the best one in terms of quality. a smart-enough LLM could be able to estimate the K value for a given query based on its complexity and, optionally, the context of the system.\n",
    "Another approach relies on the use of **Query Composition**: Starting from the original query, the LLM is (_kindly_) asked to generate a set of small, atomic sub-queries (addionally each with an associated K value) that reflects the decomposition of the original query into smaller, more manageable parts. The retrieval results for each sub-query is then merged using techniques like **Reciprocal Rank Fusion**. Alternatively, each sub-query could be answered separately and the sub-results merged by an LLM to achieve a final answer. This approach is paricularly useful when the original query is too complex to be answered in a single step, but it requires a quality LLM to achieve consistent results and adds complexity and latency to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846471da",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Query Complexity Score\n",
    "\n",
    "To estimate the **Complexity Score** for a given query, we will use different heuristics that capture a variety of aspects that reflect complexity. The following components will be considered:\n",
    "\n",
    "- **Length of the query**: The longer the query (in terms of _tokens_), the more complex it is likely to be;\n",
    "\n",
    "- **Number of unique entities in the query**: The more unique entities are mentioned in the query, the more complex it is likely to be;\n",
    "\n",
    "- **Number of different sentences and conjunctions in the query**: The more sentences and _relevant_ conjunctions (that connect two separate, meaningful clauses) are used in the query, the more complex it is likely to be;\n",
    "\n",
    "We will then normalize these components and combine them into a single score by using a **Weighted Mean** function, which will allow us to assign different weights to each heuristic based on their importance in the context of the system. The final score will be normalized to a range between 0 and 1, where a low value represents a trivial query and a high value reflects a highly complex query:\n",
    "\n",
    "\\begin{aligned}\n",
    "QCS(q) =\\ & w_{\\text{len}} \\cdot \\text{norm}(\\text{len}(q)) \\\\\n",
    "         & +\\ w_{\\text{cc}} \\cdot \\text{norm}(\\#cc(q)) \\\\\n",
    "         & +\\ w_{\\text{sent}} \\cdot \\text{norm}(\\#sent(q)) \\\\\n",
    "         & +\\ w_{\\text{ent}} \\cdot \\text{norm}(\\#ent(q))\n",
    "\\end{aligned}\n",
    "\n",
    "With $QCS(q), w_{x} \\in (0,1)$, $\\sum_{x} w_x = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8e20a",
   "metadata": {},
   "source": [
    "#### 3.1 Preliminaries to QCS\n",
    "\n",
    "To estimate the components discussed earlier, we will take advantage of the [**SpaCy** library](https://spacy.io/), which offers powerful and efficient NLP pipelines to perform operations such as _Dependency Parsing_ and _Named Entity Recognition_. Specifically, we'll use the **en_core_web_sm** pipeline, which is blazingly fast, lightweight (~12 MB of disk space) and provides a good balance between performance and accuracy for our use case.\n",
    "\n",
    "Here is what we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d37fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install --yes spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31700ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "except IOError:\n",
    "\tfrom spacy.cli.download import download\n",
    "\tdownload(\"en_core_web_sm\")\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e5a0d",
   "metadata": {},
   "source": [
    "Now that we've loaded the SpaCy models, we can try them out on a test query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Rome is the capital of Italy. Italy is known for its rich history, art (e.g. David by Michelangelo), and culture.\n",
      "********************\n",
      "Number of tokens: 26\n",
      "--------------------\n",
      "Number of sentences: 2\n",
      " Sentence 2 contains CONJUCTION ('and')\n",
      "--------------------\n",
      "Number of entities: 5\n",
      " 1. Entity: Rome, Label: GPE\n",
      " 2. Entity: Italy, Label: GPE\n",
      " 3. Entity: Italy, Label: GPE\n",
      " 4. Entity: David, Label: PERSON\n",
      " 5. Entity: Michelangelo, Label: PERSON\n"
     ]
    }
   ],
   "source": [
    "test = \"Rome is the capital of Italy. Italy is known for its rich history, art (e.g. David by Michelangelo), and culture.\"\n",
    "doc = nlp(test)\n",
    "\n",
    "print(f\"Query: {test}\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Number of tokens: {len(doc)}\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Number of sentences: {len(list(doc.sents))}\")\n",
    "for i, sent in enumerate(doc.sents):\n",
    "    if \"and\" in sent.text:\n",
    "        print(f\" Sentence {i+1} contains CONJUCTION ('and')\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Number of entities: {len(doc.ents)}\")\n",
    "for i, ent in enumerate(doc.ents):\n",
    "    print(f\" {i+1}. Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe4df8",
   "metadata": {},
   "source": [
    "We have exactly what we need. Before we proceed with the implementation, let's go over some observations:\n",
    "\n",
    "- The **query length** is a great indicator of query complexity, But it is not enough on its own: for example, the query \"_What's the capital of the country next to France_?\" isn't long, but its ambiguous and requires some reasoning to provide an answer. Another point to consider is that we are using _number of tokens_ as a measure of length instead of _number of characters_. This is because the QCS is going to be a _weighted average_ of the different components, and considering number of tokens as a length measure allows us to have a slightly more consistent measure across queries;\n",
    "  \n",
    "- We are using the number of **Distinct** entities in the query, not the total predicted number of entities. This is because we could have queries such as \"Where's Italy and what's the capital of Italy?\" that have the same entity mentioned multiple times, but since we are also considering the number of distinct clauses, this repetition shouldn't matter while calculating the QCS;\n",
    "  \n",
    "- **Sentence Segmentation** is done smartly: In the example query, we have the \"_e.g. David by Michelangelo_\" part that, while containing dots, isn't considered as separate sentences;\n",
    "  \n",
    "- Estimating the number of **Conjunctions** in a query is the trickiest part. We can't just count how many times the token \"_and_\" appears in the query. For example, the query \"What are the Q3 earnings of Johnson and Johnson?\" contains a conjunction, but it's part of a company name, so it shouldn't add complexity. What we ultimately want to count is the number of **Coordinating Conjunctions** (CCs) that connect main clauses in the query. We can use **Dependency Parsing** to achieve this.\n",
    "\n",
    "> Please note that to calculate the QCS we are only considering the \"and\" token, but the set of coordinating conjunctions for the english language consists of the _FANBOYS_ conjunctions: **For**, **And**, **Nor**, **But**, **Or**, **Yet**, **So**. This is a matter of preference that can be adjusted based on the specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020e9c8",
   "metadata": {},
   "source": [
    "#### 3.2 Implementing QCS\n",
    "\n",
    "Now that we have a clear understanding of the intuition behind QCS, we can begin implementing the main function and its components. We’ll start by building the function that calculates the number of **Relevant Coordinating Conjunctions** (CCs) in a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "def count_ccs(doc : Doc) -> float:\n",
    "    \"\"\"\n",
    "    Count the number of relevant Coordinating Conjunctions (CCs) in the phrase.\n",
    "    \"\"\"\n",
    "    cc_count = 0.0\n",
    "    for token in doc:\n",
    "        if token.text.lower() == \"and\" and token.dep_ == \"cc\":\n",
    "            head = token.head\n",
    "\n",
    "            # CASE 1. Check if 'and' connects two verbal phrases\n",
    "            # head verb could be AUX, so we check for root as well\n",
    "            if head.pos_ == \"VERB\" or head.dep_ == \"ROOT\":\n",
    "                if any(child.dep_ == \"conj\" and child.pos_ == \"VERB\"\\\n",
    "                for child in head.children):\n",
    "                    cc_count += 1\n",
    "            \n",
    "            # CASE 2. Check if 'and' has a question as a conjunct\n",
    "            # search for \"Wh-\" words in the CC subtree\n",
    "            elif any(child.dep_ == \"conj\" and \\\n",
    "                 any(t.tag_ == \"WRB\" or t.tag_ == \"WP\" for t in child.subtree) \\\n",
    "            for child in head.children):\n",
    "                cc_count += 1\n",
    "\n",
    "    return cc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b07a03",
   "metadata": {},
   "source": [
    "The `count_ccs` function looks for two main patterns in the query to identify relevant CCs:\n",
    "\n",
    "1. If the head of the token is a verb or a root, it checks if there are any **verb conjucts**. These tipically appear in the second clause (after the \"and\") of the query;\n",
    "2. If the first condition is not met, it checks wether the clause contains \"**Wh-**\" words. Their presence usually indicates that the second part of the query is a separate, complete cause (most likely a separate question).\n",
    "\n",
    "Let's test the `count_ccs` function on some example queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "63e0c334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tests passed! :)\n"
     ]
    }
   ],
   "source": [
    "def test_count_css():\n",
    "    # This should be 0, as the second clause doesn't contribute to the question\n",
    "    assert count_ccs(nlp(\"I would like to visit Rome and Italy and I don't know which one to choose?\")) == 0.0\n",
    "    # This should be 1, as the second clause is a separate question\n",
    "    assert count_ccs(nlp(\"What is the most important dish in Italy and how is it prepared traditionally?\")) == 1.0\n",
    "    # This should be 2, the first \"and\" connects two nouns\n",
    "    assert count_ccs(nlp(\"What are the Q3 earnings of Johnson and Johnson and how do they compare to the previous quarter? Also what is the highest quarterly earning and what increased sales the most?\")) == 2.0\n",
    "\n",
    "test_count_css()\n",
    "print(\"✅ All tests passed! :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca36ee1",
   "metadata": {},
   "source": [
    "The `count_css` works as expected! Before we proceed with the implementation of the main function, let's define some **Normalization Constants**. These are mandatory since we are using arbitrary values without a set range to calculate the QCS, and we want to make sure that the final QCS is normalized between 0 and 1. These constants can be tuned based on the expected query statistics, but for this example we will use some reasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6b1b4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60    # max token length for a query\n",
    "MAX_CC = 2      # max relevant conjunctions expected\n",
    "MAX_SENT = 3    # max sentences expected in a query\n",
    "MAX_ENT = 4     # max distinct entities expected\n",
    "\n",
    "MIN_K = 1      # minimum value for K\n",
    "MAX_K = 8       # maximum value for K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5d30d",
   "metadata": {},
   "source": [
    "Let's now implement the main `calculate_qcs` function, which takes a `query`string and four floats, `[len_w, cc_w, sent_w, ent_w]`, which represent the **weight** for each corresponding component (sentence length, number of CCs, number of separate sentences, number of entities). This function returns the Query Complexity Score as a `float` in the range (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e070d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_qcs(query : str,\n",
    "                  len_w : float = .3,\n",
    "                  cc_w : float = .2, \n",
    "                  sent_w : float = .3, \n",
    "                  ent_w : float = .2,) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Query Complexity Score (QCS) for a given query.\n",
    "    \"\"\"\n",
    "    if len_w + cc_w + sent_w + ent_w != 1.0:\n",
    "        raise ValueError(\"Weights must sum to 1.0\")\n",
    "\n",
    "    doc = nlp(query)\n",
    "\n",
    "    # Calculate each component\n",
    "    len_count = len(doc)\n",
    "    cc_count = count_ccs(doc)\n",
    "    sentence_count = len(list(doc.sents))\n",
    "    entity_count = len(set([ent.text for ent in doc.ents]))\n",
    "\n",
    "    # Normalize each component\n",
    "    norm_len = min(len_count / MAX_LEN, 1.0)\n",
    "    norm_cc = min(cc_count / MAX_CC, 1.0)\n",
    "    norm_sent = min(sentence_count / MAX_SENT, 1.0)\n",
    "    norm_ent = min(entity_count / MAX_ENT, 1.0)\n",
    "\n",
    "    # Return weighted sum\n",
    "    return len_w * norm_len + \\\n",
    "           cc_w * norm_cc + \\\n",
    "           sent_w * norm_sent + \\\n",
    "           ent_w * norm_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68a5ea",
   "metadata": {},
   "source": [
    "Great! Let's now test this function with some example queries of increasing complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af2a8e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16999999999999998\n",
      "0.20999999999999996\n",
      "0.315\n",
      "0.715\n"
     ]
    }
   ],
   "source": [
    "print(calculate_qcs(\"Capital of Italy?\"))\n",
    "print(calculate_qcs(\"What's the capital of Italy and how big is it?\"))\n",
    "print(calculate_qcs(\"What's an important dish in Italy and how is it prepared?\"))\n",
    "print(calculate_qcs(\"I'm an exchange student and i just got here in Italy. What can you tell me about the italian culture, and what famous dishes can i eat in Rome?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b5e25",
   "metadata": {},
   "source": [
    "As you can see, the `calculate_qcs` functions is correctly estimating a QCS for each query, with complex queries achieving higher scores. Now, estimating the K value is trivial: we can use a linear function that maps the QCS to a range of K values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd726aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, ceil # the ceil function could alternatively be used here\n",
    "\n",
    "def estimate_k(query : str,\n",
    "               min_k: int = MIN_K,\n",
    "               max_k: int = MAX_K) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the K value based on the QCS.\n",
    "    \"\"\"\n",
    "    return floor(min_k + (max_k - min_k) * calculate_qcs(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8143399",
   "metadata": {},
   "source": [
    "Let's finally test the `estimate_k` function with the same queries from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3d5d0ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(estimate_k(\"Capital of Italy?\"))\n",
    "print(estimate_k(\"What's the capital of Italy and how big is it?\"))\n",
    "print(estimate_k(\"What's an important dish in Italy and how is it prepared?\"))\n",
    "print(estimate_k(\"I'm an exchange student and i just got here in Italy. What can you tell me about the italian culture, and what famous dishes can i eat in Rome?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7783b",
   "metadata": {},
   "source": [
    "We can observe that the `estimate_k` function is correctly estimating the K value for each query, with more complex queries having higher K values. This is exactly what we wanted to achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc033a",
   "metadata": {},
   "source": [
    "### 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9349b",
   "metadata": {},
   "source": [
    "The following diagram presents a high level overview of the main steps of the QCS approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfecd4",
   "metadata": {},
   "source": [
    "![QCS Diagram](https://raw.githubusercontent.com/LucaStrano/Experimental_RAG_Tech/refs/heads/main/images/1_estimating_k_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cf5b5",
   "metadata": {},
   "source": [
    "Overall, the **Query Complexity Score** offers a fast, lightweight and pratical way to dynamically estimate the complexity of a query and, subsequently, the K value to optimize the retrieval phase. Key advantages of this approach include:\n",
    "\n",
    "- Not requiring any training or fine-tuning;\n",
    "  \n",
    "- Easily adaptable to different contexts by adjusting normalization constants and component weights;\n",
    "  \n",
    "- Minmal overhead, as it relies on simple heuristics and the SpaCy library, which is optimized for performance.\n",
    "\n",
    "This approach also comes with some limitations, such as:\n",
    "\n",
    "- The considered heuristics may not be sufficient to capture the full complexity of a query;\n",
    "\n",
    "- The QCS function may not generalize well to every context, especially if the queries contain domain-specific terms of formatting that is poorly recognized by the SpaCy models;\n",
    "\n",
    "- Each language might require a different set of heuristics to consider.\n",
    "\n",
    "To get the most out of this technique, it is recommended to use it in conjunction with other techniques such as **Query Decomposition** by following this approach:\n",
    "\n",
    "1. Decompose the original query into smaller, manageable sub-queries using an LLM;\n",
    "   \n",
    "2. Estimate the QCS and K value for each sub-query;\n",
    "   \n",
    "3. Retrieve the documents for each sub-query using the estimated K value;\n",
    "   \n",
    "4. Merge the retrieved documents using techniques such as **Reciprocal Rank Fusion** or answer each sub-query separately and merge the sub-results.\n",
    "\n",
    "The QCS function could also be further improved by adding more components, such as presence of **quantifiers** (e.g. \"all\", \"some\", \"most\") or **negations** (e.g. \"not\", \"never\"), which can also affect the complexity of the query. Additionally, a small-scale empirical evaluation could be performed to compare the gains and losses of this approach with a fixed K value approach. \n",
    "\n",
    "Thank you for reading this notebook! I hope you found it useful. If you have any questions or suggestions, feel free to send me an email at **strano.lucass@gmail.com** or send me a message on [LinkedIn](https://www.linkedin.com/in/strano-lucass/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c6575",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".exp_rag_tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
